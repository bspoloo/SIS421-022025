{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15e7ced7",
   "metadata": {},
   "source": [
    "# QLoRA aplicado a un LLM conocido como TinyLlama\n",
    "En este cuadernillo se explica como se logra aplicar QLoRA a un Modelo preetrenado como lo es TinyLlama\n",
    "\n",
    "Es fundamental para QLoRA ya que mantendremos el modelo base cuantizado (frozen) mientras solo se entrenan los adaptadores LoRA en precisión completa, logrando un balance óptimo entre eficiencia de memoria y calidad del fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f5c006",
   "metadata": {},
   "source": [
    "## Importacion de librerias\n",
    "\n",
    "En esta parte importamos todas las librerias necesarias para trabajar con QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a87e83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import bitsandbytes as bnb # Para la cuantización\n",
    "from torchvision import models, transforms, datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM # Para cargar modelos de lenguaje\n",
    "from peft import LoraConfig, get_peft_model # Para QLoRA\n",
    "\n",
    "import torch\n",
    "import bitsandbytes as bnb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b691fce",
   "metadata": {},
   "source": [
    "## Carga del modelo\n",
    "\n",
    "En esta sección del código se está configurando y cargando un modelo de lenguaje grande (LLM) con cuantización de 4 bits para implementar QLoRA (Quantized Low-Rank Adaptation). Específicamente:\n",
    "\n",
    "Se utiliza el modelo \"PY007/TinyLlama-1.1B-Chat-v0.1\", que es una versión compacta de 1.1 mil millones de parámetros optimizada para chat. Este modelo sirve como base para el fine-tuning posterior.\n",
    "\n",
    "- **Cuantización a 4 bits:** El parámetro load_in_4bit=True activa la cuantización que reduce cada peso del modelo de 16 bits (float16) a solo 4 bits, disminuyendo dramáticamente el uso de memoria. Esto permite ejecutar modelos grandes en hardware con memoria limitada, como GPUs de consumo.\n",
    "\n",
    "- **Configuración de precisión:** Se establece torch_dtype=torch.float16 para usar precisión half, mientras que bnb_4bit_compute_dtype=torch.float16 especifica que los cálculos internos también usen 16 bits. El parámetro device_map=\"auto\" distribuye automáticamente las capas del modelo entre GPU y CPU según la memoria disponible.\n",
    "\n",
    "- **Optimizaciones de cuantización:** bnb_4bit_use_double_quant=True habilita doble cuantización para mayor compresión, y bnb_4bit_quant_type=\"nf4\" usa el formato NormalFloat4, que es especialmente efectivo para pesos que siguen distribuciones normales como los de los transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a6db7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Animetx\\.cache\\huggingface\\hub\\models--PY007--TinyLlama-1.1B-Chat-v0.1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "# Nombre del modelo TinyLlama\n",
    "model_name = \"PY007/TinyLlama-1.1B-Chat-v0.1\"\n",
    "\n",
    "# Cargar el tokenizador para el modelo\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Cargar el modelo con cuantización en 4 bits\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_4bit=True, # Habilitar carga en 4 bits\n",
    "    device_map=\"auto\", # Asignar automáticamente a los dispositivos disponibles\n",
    "    torch_dtype=torch.float16, # Usar float16 para eficiencia\n",
    "    bnb_4bit_compute_dtype=torch.float16, # Tipo de dato para cálculos\n",
    "    bnb_4bit_use_double_quant=True, # Usar doble cuantización para mejor precisión\n",
    "    bnb_4bit_quant_type=\"nf4\" # Tipo de cuantización (Normal Float 4)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0db31d4",
   "metadata": {},
   "source": [
    "### Configuracoin de los adaptadores LoRA\n",
    "\n",
    "Se crea un objeto LoraConfig que define los parámetros clave para la adaptación de bajo rango:\n",
    "\n",
    "- **r=8:** Este es el rango de las matrices de bajo rango A y B. Un valor de 8 significa que cada adaptador LoRA tendrá matrices de dimensión reducida, lo que mantiene el número de parámetros entrenables muy bajo comparado con el fine-tuning completo.\n",
    "\n",
    "- **lora_alpha=32:** Factor de escalado que controla la influencia de los adaptadores LoRA. La relación alpha/r (32/8 = 4) determina qué tanto impacto tendrán las adaptaciones sobre el comportamiento original del modelo.\n",
    "\n",
    "- **target_modules=[\"q_proj\",\"v_proj\"]:** Especifica exactamente qué capas del transformer serán adaptadas. En este caso, solo las proyecciones de query (q_proj) y value (v_proj) del mecanismo de atención, que son las más críticas para el comportamiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d94af226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                 # rango bajo para adaptadores\n",
    "    lora_alpha=32,       # escala de los adaptadores\n",
    "    target_modules=[\"q_proj\",\"v_proj\"],  # capas donde aplicar LoRA, aqui en las proyecciones de query y value\n",
    "    lora_dropout=0.1, # dropout para regularización\n",
    "    bias=\"none\",       # no adaptar sesgos, solo pesos\n",
    "    task_type=\"CAUSAL_LM\" # tipo de tarea\n",
    ")\n",
    "\n",
    "# Aplicar LoRA al modelo cuantizado\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82753063",
   "metadata": {},
   "source": [
    "## Carga del Dataset\n",
    "\n",
    "Este conjunto de datos es una traducción al español de ``alpaca_data_cleaned.json``, que es una traducción al español del famoso dataset Alpaca de Stanford. Este dataset contiene instrucciones y respuestas en formato conversacional, ideal para entrenar modelos de chat en español. Sacado desde ``hugging-face`` https://huggingface.co/datasets/bertin-project/alpaca-spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11ecda1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Animetx\\.cache\\huggingface\\hub\\datasets--bertin-project--alpaca-spanish. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Generating train split: 100%|██████████| 51942/51942 [00:00<00:00, 855952.83 examples/s]\n",
      "Map: 100%|██████████| 51942/51942 [00:05<00:00, 10090.42 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Dataset de prueba\n",
    "dataset = load_dataset(\"bertin-project/alpaca-spanish\")\n",
    "\n",
    "# Usar solo el conjunto de entrenamiento\n",
    "train_dataset = dataset[\"train\"]\n",
    "\n",
    "# Preprocesamiento de los datos\n",
    "def preprocess(examples):\n",
    "    # Formatear prompt + respuesta para cada ejemplo en el batch\n",
    "    prompts = [f\"### Human: {inst}\\n### Assistant: {out}\" \n",
    "               for inst, out in zip(examples['instruction'], examples['output'])] # Crear prompts\n",
    "    tokenized = tokenizer(prompts, truncation=True, padding=\"max_length\", max_length=128) # Tokenizar\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy() # Usar input_ids como labels\n",
    "    return tokenized\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess, batched=True) # Preprocesar el dataset\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"]) # Formatear para PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc11412e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([    1,   835, 12968, 29901, 18613,  2182, 29948, 28711, 25348, 29973,\n",
      "           13,  2277, 29937,  4007, 22137, 29901, 25348, 28711,  3976, 13321,\n",
      "          553,  2251,   381,  1091,   265,  1682,   280,  1417, 29889, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor([    1,   835, 12968, 29901, 18613,  2182, 29948, 28711, 25348, 29973,\n",
      "           13,  2277, 29937,  4007, 22137, 29901, 25348, 28711,  3976, 13321,\n",
      "          553,  2251,   381,  1091,   265,  1682,   280,  1417, 29889, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000])}\n"
     ]
    }
   ],
   "source": [
    "# Mostrar un ejemplo\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cbe025",
   "metadata": {},
   "source": [
    "## Entrenamiento del Modelo\n",
    "\n",
    "En esta sección del se configura y ejecuta el entrenamiento del modelo QLoRA. Específicamente:\n",
    "\n",
    "Configuración de argumentos de entrenamiento: Se crea un objeto TrainingArguments que define todos los hiperparámetros y configuraciones para el proceso de entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6a6855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 18:02, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>5.764700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>5.836300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>5.124900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>5.011300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.614000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>4.299900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>4.446800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>4.528800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>4.303000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>4.306700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>4.349000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>4.139500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>4.041600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>4.151900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>4.156800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>3.812100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>3.968900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>4.025800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>3.951600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.936600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>4.091700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>3.762800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>3.951400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>4.072400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>3.967500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>3.823900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>3.687400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>3.774900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>3.911300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.818200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>3.642100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>3.886400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>3.821300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>3.647900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>3.952100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>3.644300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>3.831700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>3.834400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>3.875800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.938800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>3.652100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>3.536300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>3.524000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>3.732900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>3.845600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>3.864300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>3.800900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>3.942300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>3.806400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.676100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>3.569900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>3.615300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>4.072100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>3.996400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>3.438300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>3.738600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>3.939800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>4.024000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>3.841300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.739100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>3.778100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>3.748200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>3.796200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>3.680400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>3.808500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>3.779900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>3.700100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>3.563800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>3.875100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>4.115400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>3.891100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>4.115900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>3.862600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>3.753900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>3.543400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>3.790700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>3.512500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>3.873900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>3.685400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>3.822600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>3.623300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>3.858000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>3.837100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>3.938000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>3.348500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>3.886800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>3.559400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>3.950400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>3.859700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>3.675700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>3.856800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>3.780800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>3.784100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>3.635900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>3.632600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>3.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>3.938900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>3.621600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>3.741300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.611800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=3.9183070449829103, metrics={'train_runtime': 1084.4133, 'train_samples_per_second': 14.755, 'train_steps_per_second': 0.922, 'total_flos': 1.2725954543616e+16, 'train_loss': 3.9183070449829103, 'epoch': 0.30803588618074007})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./tinyllama-lora\", # directorio de salida\n",
    "    per_device_train_batch_size=2, # tamaño de batch por dispositivo\n",
    "    gradient_accumulation_steps=8, # acumular gradientes\n",
    "    learning_rate=2e-4, # tasa de aprendizaje\n",
    "    fp16=True,               # usar precisión mixta\n",
    "    logging_steps=10,       # pasos de registro\n",
    "    save_steps=200,         # pasos para guardar el modelo\n",
    "    save_total_limit=2,    # límite total de modelos guardados\n",
    "    max_steps=1000,        # pasos máximos de entrenamiento\n",
    ")\n",
    "# Configurar el Trainer\n",
    "trainer = Trainer(\n",
    "    model=model, # el modelo a entrenar\n",
    "    args=training_args, # argumentos de entrenamiento\n",
    "    train_dataset=train_dataset # conjunto de datos de entrenamiento\n",
    ")\n",
    "# Iniciar el entrenamiento\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deee6464",
   "metadata": {},
   "source": [
    "## Prueba del Modelo Entrenado con QLoRA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b79a0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar adaptadores LoRA\n",
    "model.save_pretrained(\"./tinyllama-lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b914315",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    }
   ],
   "source": [
    "# Cargar el modelo cuantizado y aplicar LoRA para inferencia\n",
    "from peft import PeftModel\n",
    "\n",
    "# Cargar el modelo cuantizado\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, # nombre del modelo\n",
    "    load_in_4bit=True, # cargar en 4 bits\n",
    "    device_map=\"auto\", # asignación automática de dispositivos\n",
    "    torch_dtype=torch.float16, # usar float16\n",
    "    bnb_4bit_compute_dtype=torch.float16, # tipo de dato para cálculos\n",
    "    bnb_4bit_use_double_quant=True, # usar doble cuantización\n",
    "    bnb_4bit_quant_type=\"nf4\" # tipo de cuantización\n",
    ")\n",
    "\n",
    "# Cargar los adaptadores LoRA entrenados\n",
    "model = PeftModel.from_pretrained(model, \"./tinyllama-lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c35e47e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Cargar el tokenizador\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Crear un prompt de prueba\n",
    "prompt = \"### Human: ¿Capitan Lara usara uniforme?.\\n### Assistant:\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b3703f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultado:\n",
      "### Human: ¿Capitan Lara usara uniforme?.\n",
      "### Assistant: No, Capitan Lara no usara uniforme.Si un capitan Lara usara uniforme, esto es una desafortunada y deshonrada actitud. Esto puede hacerle perder el apoyo del equipo, y puede afectar la confianza de los demás en la empresa.Es importante que los empleados sean respetuosos y respetan los derechos de los demás, y no deben utilizar conductas que afecten la estabilidad y la confianza de los demás.La seguridad de los empleados y los demás es el principal objetivo de la empresa, y es importante que los empleados sean respetuosos y respetan los derechos de los demás.Las estrategias de seguridad de la empresa incluyen la creación de un ambiente seguro y la vigilancia de los recursos.Los empleados\n"
     ]
    }
   ],
   "source": [
    "# Modo evaluación\n",
    "model.eval()\n",
    "\n",
    "# Generación de texto\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=200,\n",
    "    do_sample=True,\n",
    "    top_p=0.9,\n",
    "    temperature=0.7,\n",
    "    pad_token_id=tokenizer.eos_token_id,  # importante para modelos pequeños\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "# Decodificar tokens\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Resultado:\")\n",
    "print(generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
