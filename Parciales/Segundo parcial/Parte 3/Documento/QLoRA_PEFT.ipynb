{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15e7ced7",
   "metadata": {},
   "source": [
    "# QLoRA aplicado a un LLM conocido como TinyLlama\n",
    "En este cuadernillo se explica como se logra aplicar QLoRA a un Modelo preetrenado como lo es TinyLlama\n",
    "\n",
    "Es fundamental para QLoRA ya que mantendremos el modelo base cuantizado (frozen) mientras solo se entrenan los adaptadores LoRA en precisión completa, logrando un balance óptimo entre eficiencia de memoria y calidad del fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f5c006",
   "metadata": {},
   "source": [
    "## Importacion de librerias\n",
    "\n",
    "En esta parte importamos todas las librerias necesarias para trabajar con QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a87e83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programs\\Python\\3.12.7\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import bitsandbytes as bnb # Para la cuantización\n",
    "from torchvision import models, transforms, datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM # Para cargar modelos de lenguaje\n",
    "from peft import LoraConfig, get_peft_model # Para QLoRA\n",
    "\n",
    "import torch\n",
    "import bitsandbytes as bnb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b691fce",
   "metadata": {},
   "source": [
    "## Carga del modelo\n",
    "\n",
    "En esta sección del código se está configurando y cargando un modelo de lenguaje grande (LLM) con cuantización de 4 bits para implementar QLoRA (Quantized Low-Rank Adaptation). Específicamente:\n",
    "\n",
    "Se utiliza el modelo \"PY007/TinyLlama-1.1B-Chat-v0.1\", que es una versión compacta de 1.1 mil millones de parámetros optimizada para chat. Este modelo sirve como base para el fine-tuning posterior.\n",
    "\n",
    "- **Cuantización a 4 bits:** El parámetro load_in_4bit=True activa la cuantización que reduce cada peso del modelo de 16 bits (float16) a solo 4 bits, disminuyendo dramáticamente el uso de memoria. Esto permite ejecutar modelos grandes en hardware con memoria limitada, como GPUs de consumo.\n",
    "\n",
    "- **Configuración de precisión:** Se establece torch_dtype=torch.float16 para usar precisión half, mientras que bnb_4bit_compute_dtype=torch.float16 especifica que los cálculos internos también usen 16 bits. El parámetro device_map=\"auto\" distribuye automáticamente las capas del modelo entre GPU y CPU según la memoria disponible.\n",
    "\n",
    "- **Optimizaciones de cuantización:** bnb_4bit_use_double_quant=True habilita doble cuantización para mayor compresión, y bnb_4bit_quant_type=\"nf4\" usa el formato NormalFloat4, que es especialmente efectivo para pesos que siguen distribuciones normales como los de los transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6db7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    }
   ],
   "source": [
    "# Nombre del modelo TinyLlama\n",
    "model_name = \"PY007/TinyLlama-1.1B-Chat-v0.1\"\n",
    "\n",
    "# Cargar el tokenizador para el modelo\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Cargar el modelo con cuantización en 4 bits\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_4bit=True, # Habilitar carga en 4 bits\n",
    "    device_map=\"auto\", # Asignar automáticamente a los dispositivos disponibles\n",
    "    torch_dtype=torch.float16, # Usar float16 para eficiencia\n",
    "    bnb_4bit_compute_dtype=torch.float16, # Tipo de dato para cálculos\n",
    "    bnb_4bit_use_double_quant=True, # Usar doble cuantización para mejor precisión\n",
    "    bnb_4bit_quant_type=\"nf4\" # Tipo de cuantización (Normal Float 4)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0db31d4",
   "metadata": {},
   "source": [
    "### Configuracoin de los adaptadores LoRA\n",
    "\n",
    "Se crea un objeto LoraConfig que define los parámetros clave para la adaptación de bajo rango:\n",
    "\n",
    "- **r=8:** Este es el rango de las matrices de bajo rango A y B. Un valor de 8 significa que cada adaptador LoRA tendrá matrices de dimensión reducida, lo que mantiene el número de parámetros entrenables muy bajo comparado con el fine-tuning completo.\n",
    "\n",
    "- **lora_alpha=32:** Factor de escalado que controla la influencia de los adaptadores LoRA. La relación alpha/r (32/8 = 4) determina qué tanto impacto tendrán las adaptaciones sobre el comportamiento original del modelo.\n",
    "\n",
    "- **target_modules=[\"q_proj\",\"v_proj\"]:** Especifica exactamente qué capas del transformer serán adaptadas. En este caso, solo las proyecciones de query (q_proj) y value (v_proj) del mecanismo de atención, que son las más críticas para el comportamiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94af226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                 # rango bajo para adaptadores\n",
    "    lora_alpha=32,       # escala de los adaptadores\n",
    "    target_modules=[\"q_proj\",\"v_proj\"],  # capas donde aplicar LoRA \n",
    "    lora_dropout=0.1, # dropout para regularización\n",
    "    bias=\"none\",       # no adaptar sesgos\n",
    "    task_type=\"CAUSAL_LM\" # tipo de tarea\n",
    ")\n",
    "\n",
    "# Aplicar LoRA al modelo cuantizado\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82753063",
   "metadata": {},
   "source": [
    "## Carga del Dataset\n",
    "\n",
    "Este conjunto de datos es una traducción al español de ``alpaca_data_cleaned.json``, que es una traducción al español del famoso dataset Alpaca de Stanford. Este dataset contiene instrucciones y respuestas en formato conversacional, ideal para entrenar modelos de chat en español. Sacado desde ``hugging-face`` https://huggingface.co/datasets/bertin-project/alpaca-spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ecda1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 51942/51942 [00:03<00:00, 13286.85 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Dataset de prueba\n",
    "dataset = load_dataset(\"bertin-project/alpaca-spanish\")\n",
    "\n",
    "# Usar solo el conjunto de entrenamiento\n",
    "train_dataset = dataset[\"train\"]\n",
    "\n",
    "# Preprocesamiento de los datos\n",
    "def preprocess(examples):\n",
    "    # Formatear prompt + respuesta para cada ejemplo en el batch\n",
    "    prompts = [f\"### Human: {inst}\\n### Assistant: {out}\" \n",
    "               for inst, out in zip(examples['instruction'], examples['output'])] # Crear prompts\n",
    "    tokenized = tokenizer(prompts, truncation=True, padding=\"max_length\", max_length=128) # Tokenizar\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy() # Usar input_ids como labels\n",
    "    return tokenized\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess, batched=True) # Preprocesar el dataset\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"]) # Formatear para PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc11412e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([    1,   835, 12968, 29901, 18613,  2182, 29948, 28711, 25348, 29973,\n",
      "           13,  2277, 29937,  4007, 22137, 29901, 25348, 28711,  3976, 13321,\n",
      "          553,  2251,   381,  1091,   265,  1682,   280,  1417, 29889, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor([    1,   835, 12968, 29901, 18613,  2182, 29948, 28711, 25348, 29973,\n",
      "           13,  2277, 29937,  4007, 22137, 29901, 25348, 28711,  3976, 13321,\n",
      "          553,  2251,   381,  1091,   265,  1682,   280,  1417, 29889, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000])}\n"
     ]
    }
   ],
   "source": [
    "# Mostrar un ejemplo\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cbe025",
   "metadata": {},
   "source": [
    "## Entrenamiento del Modelo\n",
    "\n",
    "En esta sección del se configura y ejecuta el entrenamiento del modelo QLoRA. Específicamente:\n",
    "\n",
    "Configuración de argumentos de entrenamiento: Se crea un objeto TrainingArguments que define todos los hiperparámetros y configuraciones para el proceso de entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6a6855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 18:02, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>5.764700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>5.836300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>5.124900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>5.011300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.614000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>4.299900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>4.446800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>4.528800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>4.303000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>4.306700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>4.349000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>4.139500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>4.041600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>4.151900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>4.156800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>3.812100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>3.968900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>4.025800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>3.951600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.936600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>4.091700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>3.762800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>3.951400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>4.072400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>3.967500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>3.823900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>3.687400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>3.774900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>3.911300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.818200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>3.642100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>3.886400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>3.821300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>3.647900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>3.952100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>3.644300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>3.831700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>3.834400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>3.875800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.938800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>3.652100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>3.536300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>3.524000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>3.732900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>3.845600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>3.864300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>3.800900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>3.942300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>3.806400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.676100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>3.569900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>3.615300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>4.072100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>3.996400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>3.438300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>3.738600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>3.939800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>4.024000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>3.841300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.739100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>3.778100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>3.748200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>3.796200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>3.680400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>3.808500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>3.779900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>3.700100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>3.563800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>3.875100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>4.115400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>3.891100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>4.115900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>3.862600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>3.753900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>3.543400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>3.790700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>3.512500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>3.873900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>3.685400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>3.822600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>3.623300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>3.858000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>3.837100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>3.938000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>3.348500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>3.886800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>3.559400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>3.950400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>3.859700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>3.675700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>3.856800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>3.780800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>3.784100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>3.635900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>3.632600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>3.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>3.938900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>3.621600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>3.741300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.611800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=3.9183070449829103, metrics={'train_runtime': 1084.4133, 'train_samples_per_second': 14.755, 'train_steps_per_second': 0.922, 'total_flos': 1.2725954543616e+16, 'train_loss': 3.9183070449829103, 'epoch': 0.30803588618074007})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./tinyllama-lora\", # directorio de salida\n",
    "    per_device_train_batch_size=2, # tamaño de batch por dispositivo\n",
    "    gradient_accumulation_steps=8, # acumular gradientes\n",
    "    learning_rate=2e-4, # tasa de aprendizaje\n",
    "    fp16=True,               # usar precisión mixta\n",
    "    logging_steps=10,       # pasos de registro\n",
    "    save_steps=200,         # pasos para guardar el modelo\n",
    "    save_total_limit=2,    # límite total de modelos guardados\n",
    "    max_steps=1000,        # pasos máximos de entrenamiento\n",
    ")\n",
    "# Configurar el Trainer\n",
    "trainer = Trainer(\n",
    "    model=model, # el modelo a entrenar\n",
    "    args=training_args, # argumentos de entrenamiento\n",
    "    train_dataset=train_dataset # conjunto de datos de entrenamiento\n",
    ")\n",
    "# Iniciar el entrenamiento\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deee6464",
   "metadata": {},
   "source": [
    "## Prueba del Modelo Entrenado con QLoRA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b79a0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar adaptadores LoRA\n",
    "model.save_pretrained(\"./tinyllama-lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b914315",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    }
   ],
   "source": [
    "# Cargar el modelo cuantizado y aplicar LoRA para inferencia\n",
    "from peft import PeftModel\n",
    "\n",
    "# Cargar el modelo cuantizado\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, # nombre del modelo\n",
    "    load_in_4bit=True, # cargar en 4 bits\n",
    "    device_map=\"auto\", # asignación automática de dispositivos\n",
    "    torch_dtype=torch.float16, # usar float16\n",
    "    bnb_4bit_compute_dtype=torch.float16, # tipo de dato para cálculos\n",
    "    bnb_4bit_use_double_quant=True, # usar doble cuantización\n",
    "    bnb_4bit_quant_type=\"nf4\" # tipo de cuantización\n",
    ")\n",
    "\n",
    "# Cargar los adaptadores LoRA entrenados\n",
    "model = PeftModel.from_pretrained(model, \"./tinyllama-lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35e47e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Cargar el tokenizador\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Crear un prompt de prueba\n",
    "prompt = \"### Human: Que es la IA.\\n### Assistant:\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3703f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💡 Resultado:\n",
      "### Human: Que es la IA.\n",
      "### Assistant: La IA es un tipo de entidad que se utiliza para procesar datos y realizar tareas con base en datos. En otras palabras, es una entidad que se puede programar para realizar tareas con base en datos. La IA se utiliza para procesar datos y realizar tareas con base en datos, como buscar información de recursos, procesamiento de datos, análisis de datos y creación de modelos. Mientras que las tareas tradicionales utilizan una serie de herramientas y programas de software, la IA utiliza tecnologías de inteligencia artificial para realizar tareas de forma automática. The Pioneer 100th Anniversary Limited Edition Edition is a limited edition of 300 copies, each of which will be signed by the entire band. This special edition includes a 180g vinyl, a 1\n"
     ]
    }
   ],
   "source": [
    "# Modo evaluación\n",
    "model.eval()\n",
    "\n",
    "# Generación de texto\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=200,\n",
    "    do_sample=True,\n",
    "    top_p=0.9,\n",
    "    temperature=0.7,\n",
    "    pad_token_id=tokenizer.eos_token_id,  # importante para modelos pequeños\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "# Decodificar tokens\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Resultado:\")\n",
    "print(generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
