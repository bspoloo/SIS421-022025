{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.30803588618074007,
  "eval_steps": 500,
  "global_step": 1000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0030803588618074007,
      "grad_norm": 4.432524681091309,
      "learning_rate": 0.0001984,
      "loss": 5.7647,
      "step": 10
    },
    {
      "epoch": 0.006160717723614801,
      "grad_norm": 2.542088508605957,
      "learning_rate": 0.0001964,
      "loss": 5.8363,
      "step": 20
    },
    {
      "epoch": 0.009241076585422201,
      "grad_norm": 2.048659086227417,
      "learning_rate": 0.0001944,
      "loss": 5.1249,
      "step": 30
    },
    {
      "epoch": 0.012321435447229603,
      "grad_norm": 1.7799677848815918,
      "learning_rate": 0.00019240000000000001,
      "loss": 5.0113,
      "step": 40
    },
    {
      "epoch": 0.015401794309037002,
      "grad_norm": 1.5591448545455933,
      "learning_rate": 0.0001904,
      "loss": 4.614,
      "step": 50
    },
    {
      "epoch": 0.018482153170844402,
      "grad_norm": 1.373574137687683,
      "learning_rate": 0.0001884,
      "loss": 4.2999,
      "step": 60
    },
    {
      "epoch": 0.021562512032651804,
      "grad_norm": 1.2204526662826538,
      "learning_rate": 0.00018640000000000003,
      "loss": 4.4468,
      "step": 70
    },
    {
      "epoch": 0.024642870894459205,
      "grad_norm": 1.281365156173706,
      "learning_rate": 0.0001844,
      "loss": 4.5288,
      "step": 80
    },
    {
      "epoch": 0.027723229756266603,
      "grad_norm": 1.202903389930725,
      "learning_rate": 0.00018240000000000002,
      "loss": 4.303,
      "step": 90
    },
    {
      "epoch": 0.030803588618074005,
      "grad_norm": 1.5692038536071777,
      "learning_rate": 0.00018040000000000002,
      "loss": 4.3067,
      "step": 100
    },
    {
      "epoch": 0.0338839474798814,
      "grad_norm": 1.33564031124115,
      "learning_rate": 0.0001784,
      "loss": 4.349,
      "step": 110
    },
    {
      "epoch": 0.036964306341688805,
      "grad_norm": 1.5035841464996338,
      "learning_rate": 0.0001764,
      "loss": 4.1395,
      "step": 120
    },
    {
      "epoch": 0.040044665203496206,
      "grad_norm": 1.2794097661972046,
      "learning_rate": 0.0001744,
      "loss": 4.0416,
      "step": 130
    },
    {
      "epoch": 0.04312502406530361,
      "grad_norm": 1.5350977182388306,
      "learning_rate": 0.00017240000000000002,
      "loss": 4.1519,
      "step": 140
    },
    {
      "epoch": 0.04620538292711101,
      "grad_norm": 1.9851422309875488,
      "learning_rate": 0.0001704,
      "loss": 4.1568,
      "step": 150
    },
    {
      "epoch": 0.04928574178891841,
      "grad_norm": 1.9541555643081665,
      "learning_rate": 0.0001684,
      "loss": 3.8121,
      "step": 160
    },
    {
      "epoch": 0.05236610065072581,
      "grad_norm": 2.62756609916687,
      "learning_rate": 0.0001664,
      "loss": 3.9689,
      "step": 170
    },
    {
      "epoch": 0.05544645951253321,
      "grad_norm": 1.434242844581604,
      "learning_rate": 0.0001644,
      "loss": 4.0258,
      "step": 180
    },
    {
      "epoch": 0.05852681837434061,
      "grad_norm": 1.1698477268218994,
      "learning_rate": 0.00016240000000000002,
      "loss": 3.9516,
      "step": 190
    },
    {
      "epoch": 0.06160717723614801,
      "grad_norm": 1.8436936140060425,
      "learning_rate": 0.00016040000000000002,
      "loss": 3.9366,
      "step": 200
    },
    {
      "epoch": 0.06468753609795541,
      "grad_norm": 1.523763656616211,
      "learning_rate": 0.00015840000000000003,
      "loss": 4.0917,
      "step": 210
    },
    {
      "epoch": 0.0677678949597628,
      "grad_norm": 1.6754896640777588,
      "learning_rate": 0.0001564,
      "loss": 3.7628,
      "step": 220
    },
    {
      "epoch": 0.07084825382157021,
      "grad_norm": 1.06339693069458,
      "learning_rate": 0.0001544,
      "loss": 3.9514,
      "step": 230
    },
    {
      "epoch": 0.07392861268337761,
      "grad_norm": 1.540382742881775,
      "learning_rate": 0.00015240000000000002,
      "loss": 4.0724,
      "step": 240
    },
    {
      "epoch": 0.07700897154518502,
      "grad_norm": 3.1908931732177734,
      "learning_rate": 0.0001504,
      "loss": 3.9675,
      "step": 250
    },
    {
      "epoch": 0.08008933040699241,
      "grad_norm": 1.500636339187622,
      "learning_rate": 0.0001484,
      "loss": 3.8239,
      "step": 260
    },
    {
      "epoch": 0.08316968926879982,
      "grad_norm": 1.3520818948745728,
      "learning_rate": 0.0001464,
      "loss": 3.6874,
      "step": 270
    },
    {
      "epoch": 0.08625004813060722,
      "grad_norm": 1.9093847274780273,
      "learning_rate": 0.0001444,
      "loss": 3.7749,
      "step": 280
    },
    {
      "epoch": 0.08933040699241461,
      "grad_norm": 3.7060887813568115,
      "learning_rate": 0.0001424,
      "loss": 3.9113,
      "step": 290
    },
    {
      "epoch": 0.09241076585422202,
      "grad_norm": 1.9244135618209839,
      "learning_rate": 0.0001404,
      "loss": 3.8182,
      "step": 300
    },
    {
      "epoch": 0.09549112471602941,
      "grad_norm": 1.2390804290771484,
      "learning_rate": 0.0001384,
      "loss": 3.6421,
      "step": 310
    },
    {
      "epoch": 0.09857148357783682,
      "grad_norm": 1.9889181852340698,
      "learning_rate": 0.0001364,
      "loss": 3.8864,
      "step": 320
    },
    {
      "epoch": 0.10165184243964422,
      "grad_norm": 2.1663081645965576,
      "learning_rate": 0.00013440000000000001,
      "loss": 3.8213,
      "step": 330
    },
    {
      "epoch": 0.10473220130145162,
      "grad_norm": 1.963897466659546,
      "learning_rate": 0.00013240000000000002,
      "loss": 3.6479,
      "step": 340
    },
    {
      "epoch": 0.10781256016325902,
      "grad_norm": 2.293027639389038,
      "learning_rate": 0.0001304,
      "loss": 3.9521,
      "step": 350
    },
    {
      "epoch": 0.11089291902506641,
      "grad_norm": 3.099317789077759,
      "learning_rate": 0.0001284,
      "loss": 3.6443,
      "step": 360
    },
    {
      "epoch": 0.11397327788687382,
      "grad_norm": 3.7791550159454346,
      "learning_rate": 0.0001264,
      "loss": 3.8317,
      "step": 370
    },
    {
      "epoch": 0.11705363674868122,
      "grad_norm": 2.9393041133880615,
      "learning_rate": 0.00012440000000000002,
      "loss": 3.8344,
      "step": 380
    },
    {
      "epoch": 0.12013399561048863,
      "grad_norm": 1.6313896179199219,
      "learning_rate": 0.0001224,
      "loss": 3.8758,
      "step": 390
    },
    {
      "epoch": 0.12321435447229602,
      "grad_norm": 1.6970603466033936,
      "learning_rate": 0.0001204,
      "loss": 3.9388,
      "step": 400
    },
    {
      "epoch": 0.12629471333410341,
      "grad_norm": 2.752017021179199,
      "learning_rate": 0.0001184,
      "loss": 3.6521,
      "step": 410
    },
    {
      "epoch": 0.12937507219591082,
      "grad_norm": 2.584059238433838,
      "learning_rate": 0.0001164,
      "loss": 3.5363,
      "step": 420
    },
    {
      "epoch": 0.13245543105771823,
      "grad_norm": 4.099057197570801,
      "learning_rate": 0.0001144,
      "loss": 3.524,
      "step": 430
    },
    {
      "epoch": 0.1355357899195256,
      "grad_norm": 3.3107972145080566,
      "learning_rate": 0.00011240000000000002,
      "loss": 3.7329,
      "step": 440
    },
    {
      "epoch": 0.13861614878133302,
      "grad_norm": 2.3846869468688965,
      "learning_rate": 0.00011040000000000001,
      "loss": 3.8456,
      "step": 450
    },
    {
      "epoch": 0.14169650764314043,
      "grad_norm": 2.1993820667266846,
      "learning_rate": 0.00010840000000000002,
      "loss": 3.8643,
      "step": 460
    },
    {
      "epoch": 0.14477686650494784,
      "grad_norm": 2.597942352294922,
      "learning_rate": 0.00010640000000000001,
      "loss": 3.8009,
      "step": 470
    },
    {
      "epoch": 0.14785722536675522,
      "grad_norm": 1.8977400064468384,
      "learning_rate": 0.0001044,
      "loss": 3.9423,
      "step": 480
    },
    {
      "epoch": 0.15093758422856263,
      "grad_norm": 1.8818373680114746,
      "learning_rate": 0.00010240000000000001,
      "loss": 3.8064,
      "step": 490
    },
    {
      "epoch": 0.15401794309037004,
      "grad_norm": 1.690852165222168,
      "learning_rate": 0.0001004,
      "loss": 3.6761,
      "step": 500
    },
    {
      "epoch": 0.15709830195217742,
      "grad_norm": 1.4572831392288208,
      "learning_rate": 9.84e-05,
      "loss": 3.5699,
      "step": 510
    },
    {
      "epoch": 0.16017866081398482,
      "grad_norm": 1.320181131362915,
      "learning_rate": 9.64e-05,
      "loss": 3.6153,
      "step": 520
    },
    {
      "epoch": 0.16325901967579223,
      "grad_norm": 2.8973381519317627,
      "learning_rate": 9.44e-05,
      "loss": 4.0721,
      "step": 530
    },
    {
      "epoch": 0.16633937853759964,
      "grad_norm": 1.2704840898513794,
      "learning_rate": 9.240000000000001e-05,
      "loss": 3.9964,
      "step": 540
    },
    {
      "epoch": 0.16941973739940702,
      "grad_norm": 3.374621629714966,
      "learning_rate": 9.04e-05,
      "loss": 3.4383,
      "step": 550
    },
    {
      "epoch": 0.17250009626121443,
      "grad_norm": 1.1447434425354004,
      "learning_rate": 8.840000000000001e-05,
      "loss": 3.7386,
      "step": 560
    },
    {
      "epoch": 0.17558045512302184,
      "grad_norm": 1.7284331321716309,
      "learning_rate": 8.64e-05,
      "loss": 3.9398,
      "step": 570
    },
    {
      "epoch": 0.17866081398482922,
      "grad_norm": 1.6368032693862915,
      "learning_rate": 8.44e-05,
      "loss": 4.024,
      "step": 580
    },
    {
      "epoch": 0.18174117284663663,
      "grad_norm": 2.0157299041748047,
      "learning_rate": 8.24e-05,
      "loss": 3.8413,
      "step": 590
    },
    {
      "epoch": 0.18482153170844404,
      "grad_norm": 1.8570516109466553,
      "learning_rate": 8.04e-05,
      "loss": 3.7391,
      "step": 600
    },
    {
      "epoch": 0.18790189057025145,
      "grad_norm": 2.2981808185577393,
      "learning_rate": 7.840000000000001e-05,
      "loss": 3.7781,
      "step": 610
    },
    {
      "epoch": 0.19098224943205883,
      "grad_norm": 1.4783217906951904,
      "learning_rate": 7.64e-05,
      "loss": 3.7482,
      "step": 620
    },
    {
      "epoch": 0.19406260829386623,
      "grad_norm": 1.6168673038482666,
      "learning_rate": 7.44e-05,
      "loss": 3.7962,
      "step": 630
    },
    {
      "epoch": 0.19714296715567364,
      "grad_norm": 1.1175965070724487,
      "learning_rate": 7.24e-05,
      "loss": 3.6804,
      "step": 640
    },
    {
      "epoch": 0.20022332601748102,
      "grad_norm": 3.6346354484558105,
      "learning_rate": 7.04e-05,
      "loss": 3.8085,
      "step": 650
    },
    {
      "epoch": 0.20330368487928843,
      "grad_norm": 2.809558868408203,
      "learning_rate": 6.840000000000001e-05,
      "loss": 3.7799,
      "step": 660
    },
    {
      "epoch": 0.20638404374109584,
      "grad_norm": 3.134127616882324,
      "learning_rate": 6.64e-05,
      "loss": 3.7001,
      "step": 670
    },
    {
      "epoch": 0.20946440260290325,
      "grad_norm": 3.411482572555542,
      "learning_rate": 6.440000000000001e-05,
      "loss": 3.5638,
      "step": 680
    },
    {
      "epoch": 0.21254476146471063,
      "grad_norm": 2.02169132232666,
      "learning_rate": 6.24e-05,
      "loss": 3.8751,
      "step": 690
    },
    {
      "epoch": 0.21562512032651804,
      "grad_norm": 1.8997008800506592,
      "learning_rate": 6.04e-05,
      "loss": 4.1154,
      "step": 700
    },
    {
      "epoch": 0.21870547918832545,
      "grad_norm": 1.6639717817306519,
      "learning_rate": 5.8399999999999997e-05,
      "loss": 3.8911,
      "step": 710
    },
    {
      "epoch": 0.22178583805013283,
      "grad_norm": 2.066169023513794,
      "learning_rate": 5.6399999999999995e-05,
      "loss": 4.1159,
      "step": 720
    },
    {
      "epoch": 0.22486619691194024,
      "grad_norm": 1.7812780141830444,
      "learning_rate": 5.440000000000001e-05,
      "loss": 3.8626,
      "step": 730
    },
    {
      "epoch": 0.22794655577374764,
      "grad_norm": 2.714682102203369,
      "learning_rate": 5.2400000000000007e-05,
      "loss": 3.7539,
      "step": 740
    },
    {
      "epoch": 0.23102691463555505,
      "grad_norm": 2.5505850315093994,
      "learning_rate": 5.0400000000000005e-05,
      "loss": 3.5434,
      "step": 750
    },
    {
      "epoch": 0.23410727349736243,
      "grad_norm": 3.5304384231567383,
      "learning_rate": 4.8400000000000004e-05,
      "loss": 3.7907,
      "step": 760
    },
    {
      "epoch": 0.23718763235916984,
      "grad_norm": 3.4479730129241943,
      "learning_rate": 4.64e-05,
      "loss": 3.5125,
      "step": 770
    },
    {
      "epoch": 0.24026799122097725,
      "grad_norm": 1.8195691108703613,
      "learning_rate": 4.44e-05,
      "loss": 3.8739,
      "step": 780
    },
    {
      "epoch": 0.24334835008278463,
      "grad_norm": 1.9131743907928467,
      "learning_rate": 4.24e-05,
      "loss": 3.6854,
      "step": 790
    },
    {
      "epoch": 0.24642870894459204,
      "grad_norm": 2.86612868309021,
      "learning_rate": 4.0400000000000006e-05,
      "loss": 3.8226,
      "step": 800
    },
    {
      "epoch": 0.24950906780639945,
      "grad_norm": 1.4335720539093018,
      "learning_rate": 3.8400000000000005e-05,
      "loss": 3.6233,
      "step": 810
    },
    {
      "epoch": 0.25258942666820683,
      "grad_norm": 1.6168979406356812,
      "learning_rate": 3.6400000000000004e-05,
      "loss": 3.858,
      "step": 820
    },
    {
      "epoch": 0.25566978553001424,
      "grad_norm": 1.8888684511184692,
      "learning_rate": 3.4399999999999996e-05,
      "loss": 3.8371,
      "step": 830
    },
    {
      "epoch": 0.25875014439182165,
      "grad_norm": 2.0578770637512207,
      "learning_rate": 3.24e-05,
      "loss": 3.938,
      "step": 840
    },
    {
      "epoch": 0.26183050325362905,
      "grad_norm": 1.0513044595718384,
      "learning_rate": 3.04e-05,
      "loss": 3.3485,
      "step": 850
    },
    {
      "epoch": 0.26491086211543646,
      "grad_norm": 1.398290753364563,
      "learning_rate": 2.84e-05,
      "loss": 3.8868,
      "step": 860
    },
    {
      "epoch": 0.26799122097724387,
      "grad_norm": 1.2511491775512695,
      "learning_rate": 2.64e-05,
      "loss": 3.5594,
      "step": 870
    },
    {
      "epoch": 0.2710715798390512,
      "grad_norm": 1.826815128326416,
      "learning_rate": 2.44e-05,
      "loss": 3.9504,
      "step": 880
    },
    {
      "epoch": 0.27415193870085863,
      "grad_norm": 1.25568425655365,
      "learning_rate": 2.2400000000000002e-05,
      "loss": 3.8597,
      "step": 890
    },
    {
      "epoch": 0.27723229756266604,
      "grad_norm": 1.2362123727798462,
      "learning_rate": 2.04e-05,
      "loss": 3.6757,
      "step": 900
    },
    {
      "epoch": 0.28031265642447345,
      "grad_norm": 1.1623947620391846,
      "learning_rate": 1.84e-05,
      "loss": 3.8568,
      "step": 910
    },
    {
      "epoch": 0.28339301528628086,
      "grad_norm": 1.0654070377349854,
      "learning_rate": 1.6400000000000002e-05,
      "loss": 3.7808,
      "step": 920
    },
    {
      "epoch": 0.28647337414808827,
      "grad_norm": 1.154908537864685,
      "learning_rate": 1.44e-05,
      "loss": 3.7841,
      "step": 930
    },
    {
      "epoch": 0.2895537330098957,
      "grad_norm": 1.5083732604980469,
      "learning_rate": 1.24e-05,
      "loss": 3.6359,
      "step": 940
    },
    {
      "epoch": 0.29263409187170303,
      "grad_norm": 1.1276894807815552,
      "learning_rate": 1.04e-05,
      "loss": 3.6326,
      "step": 950
    },
    {
      "epoch": 0.29571445073351044,
      "grad_norm": 1.219424843788147,
      "learning_rate": 8.400000000000001e-06,
      "loss": 3.93,
      "step": 960
    },
    {
      "epoch": 0.29879480959531785,
      "grad_norm": 2.0338804721832275,
      "learning_rate": 6.4000000000000006e-06,
      "loss": 3.9389,
      "step": 970
    },
    {
      "epoch": 0.30187516845712525,
      "grad_norm": 1.1135084629058838,
      "learning_rate": 4.4e-06,
      "loss": 3.6216,
      "step": 980
    },
    {
      "epoch": 0.30495552731893266,
      "grad_norm": 1.3849825859069824,
      "learning_rate": 2.4000000000000003e-06,
      "loss": 3.7413,
      "step": 990
    },
    {
      "epoch": 0.30803588618074007,
      "grad_norm": 1.072157621383667,
      "learning_rate": 4.0000000000000003e-07,
      "loss": 3.6118,
      "step": 1000
    }
  ],
  "logging_steps": 10,
  "max_steps": 1000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 200,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.2725954543616e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
